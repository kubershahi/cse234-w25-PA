We experimented with several values of max_tokens and num_speculative_tokens and noticed that as the maximum number of tokens increases, a higher value for the speculative tokens generated becomes more feasible. This is because for a larger number of tokens generated (max_tokens) there is more context for the draft model and more context results in similar predictions between the draft and target model. This makes the predictions from the draft model more likely to be accepted by the target model.

Furthermore, we noticed that as the number of speculative tokens increases there is a less likely chance for a speedup. This is because there is a lower likelihood of the generated speculative tokens to match the target model's predictions, so the target model instead generates these to rectify the mistakes made by the draft model. 

Obviously, there is a trade-off; we don't want to choose too low of a speculative token count because this is equivalent to essentially generating each token from the target model, which is what speculative decoding aims to avoid. Choosing a speculative token count that is too high will also result in similar behavior since a majority of the tokens likely won't match and the target model will have to rectify the mistakes (token predictions) made by the draft model.

As for optimizations, we vectorized the verification code as much as possible which resulted in better peformance than the non-vectorized approach, as expected. We also attempted to use model caching to avoid recomputation of hidden states, but this didn't really have much of an effect on the observed speedup number. Other than these optimizations, there weren't many other explicit optimizations that we did; we mainly just implemented the idea of speculative decoding. A majority of the speedup gains were seen when tweaking the parameters.