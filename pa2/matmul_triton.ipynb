{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740801444441,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "E-mNhUjQuxNM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740801446417,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "N9lmLw8cuxNN"
   },
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740801447218,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "eUMlpjFJuxNO"
   },
   "outputs": [],
   "source": [
    "def is_hip_mi200():\n",
    "    target = triton.runtime.driver.active.get_current_target()\n",
    "    return target.backend == 'hip' and target.arch == 'gfx90a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1740801448917,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "lBNGYaejuxNO"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PA2 Part 1: MatMul+Relu+Add Fused Optimization.\n",
    "The kernel uses several optimization techniques:\n",
    "\n",
    "    1. Shared memory tiling.\n",
    "    2. Register tiling.\n",
    "    3. Cooperative fetching.\n",
    "    4. Operator Fusion\n",
    "    5. Write cache / epilogue fusion.\n",
    "\n",
    "Fill in the missing parts (marked with TODO).\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Tiling parameters - You will need to change these to achieve better results.\n",
    "# -----------------------------------------------------------------------------\n",
    "BLOCK_M = 128  # Tile size in the M dimension.\n",
    "BLOCK_N = 256 # Tile size in the N dimension.\n",
    "BLOCK_K = 16 # Tile size in the K dimension.\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Triton Kernel: Matrix Multiplication + ReLU + Add\n",
    "#\n",
    "# The kernel uses:\n",
    "#   Step 1: Tile assignment (each kernel computes a tile of C)\n",
    "#   Step 2: Shared memory tiling + Cooperative Fetching: Load tiles of A and B.\n",
    "#   Step 3: Register tiling: Use a register accumulator.\n",
    "#   Step 4: Add and ReLU fusion\n",
    "#   Step 5: Write cache/Epilogue: Write the final tile back to global memory.\n",
    "# -----------------------------------------------------------------------------\n",
    "@triton.jit\n",
    "def matmul_add_relu_kernel_fp16(\n",
    "    a_ptr, b_ptr, c_ptr, d_ptr,\n",
    "    M: tl.constexpr, N: tl.constexpr, K: tl.constexpr,\n",
    "    stride_am: tl.constexpr, stride_ak: tl.constexpr,\n",
    "    stride_bk: tl.constexpr, stride_bn: tl.constexpr,\n",
    "    stride_cm: tl.constexpr, stride_cn: tl.constexpr,\n",
    "    stride_dm: tl.constexpr, stride_dn: tl.constexpr,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 1: Tile: Assignment\n",
    "    #\n",
    "    # Each kernel instance is mapped to a tile in the output matrix C.\n",
    "    # Compute the starting indices (m_start, n_start) for this tile.\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Compute the tile indices using program_id(0) for M and program_id(1) for N.\n",
    "\n",
    "    # computing program ids to determine which block of output matrix (say C) to compute.\n",
    "    pid_m = tl.program_id(0) # row block\n",
    "    pid_n = tl.program_id(1) # column block\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 2: Register Tiling\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Initialize the accumulator \"acc\" with zeros (dtype: float16).\n",
    "\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float16)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 3: Shared Memory Tiling & Cooperative Fetching.\n",
    "    # Compute pointers to the sub-tiles of A and B that are needed to compute\n",
    "    # the current C tile. The offsets here serve to load BLOCK_SIZE_M x BLOCK_SIZE_K\n",
    "    # and BLOCK_SIZE_K x BLOCK_SIZE_N blocks from A and B respectively.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # computing row and column offsets for the current block\n",
    "    offs_am = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_bn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "\n",
    "    # creating pointers for loading A, B\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_K)):\n",
    "\n",
    "        # loading A and B blocks with masking\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n",
    "\n",
    "        # performing matrix multiplication on the current block\n",
    "        acc += tl.dot(a, b, out_dtype=tl.float16)\n",
    "\n",
    "        # moving pointers to the next block along K\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 4: Apply ReLU and Add C to the accumulator\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # creating pointers for loading matrix C\n",
    "    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)\n",
    "\n",
    "    # loading C matrix elements\n",
    "    c = tl.load(c_ptrs, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N), other=0.0)\n",
    "\n",
    "    # adding C and applying ReLU\n",
    "    acc += c\n",
    "    acc = tl.maximum(acc, 0)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Step 5: Write Cache / Epilogue Fusion: Write the computed tile to D.\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # creating pointers for storing the computed block to D\n",
    "    d_ptrs = d_ptr + (offs_am[:, None] * stride_dm + offs_bn[None, :] * stride_dn)\n",
    "\n",
    "    # storing the computed block to D\n",
    "    tl.store(d_ptrs, acc, mask=(offs_am[:, None] < M) & (offs_bn[None, :] < N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740801451856,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "u16sz-IUuxNP"
   },
   "outputs": [],
   "source": [
    "def matmul_add_relu_fp16(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Output = ReLU(A @ B + C) using fp16 precision for maximum throughput.\n",
    "    \"\"\"\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2, \"Incompatible dimensions\"\n",
    "\n",
    "    d = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # Create launch grid\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "\n",
    "    matmul_add_relu_kernel_fp16[grid](\n",
    "        a, b, c, d,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        d.stride(0), d.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K\n",
    "    )\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1740801453917,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "AJ7LlTPawPqB"
   },
   "outputs": [],
   "source": [
    "# Reference implementation using PyTorch\n",
    "def reference_matmul_add_relu(A, B, C):\n",
    "    result = torch.matmul(A, B).add(C).relu_()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1740801456623,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "B4J5ZBpOuxNP",
    "outputId": "a6e6f4dd-e327-4015-fb4d-419a8306a867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "triton_output_with_fp16_inputs=tensor([[ 0.0000,  6.1250,  0.0000,  ..., 10.0625,  0.0000,  0.0000],\n",
      "        [ 7.9102, 15.6328, 26.6094,  ..., 11.4609,  5.3750, 18.6250],\n",
      "        [ 2.7246,  0.0000,  0.0000,  ...,  0.0000, 26.0781,  0.0000],\n",
      "        ...,\n",
      "        [ 0.4448, 75.1875,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
      "        [ 6.9492,  1.1230,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [27.6094, 26.9531, 22.9219,  ..., 13.5391,  6.0508, 21.6250]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "torch_output_with_fp16_inputs=tensor([[ 0.0000,  6.1289,  0.0000,  ..., 10.0391,  0.0000,  0.0000],\n",
      "        [ 7.9102, 15.6328, 26.6250,  ..., 11.4531,  5.3945, 18.6562],\n",
      "        [ 2.7266,  0.0000,  0.0000,  ...,  0.0000, 26.1250,  0.0000],\n",
      "        ...,\n",
      "        [ 0.4316, 75.2500,  0.0000,  ..., 26.2812,  0.0000,  0.0000],\n",
      "        [ 6.9570,  1.1260,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [27.6406, 26.9531, 22.9375,  ..., 13.5625,  6.0391, 21.6406]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Accuracy Tests\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
    "    b = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
    "    c = torch.randn((512, 512), device=torch.device(\"cuda\"), dtype=torch.float16)\n",
    "    triton_output = matmul_add_relu_fp16(a, b, c)\n",
    "    torch_output = reference_matmul_add_relu(a, b, c)\n",
    "    print(f\"triton_output_with_fp16_inputs={triton_output}\")\n",
    "    print(f\"torch_output_with_fp16_inputs={torch_output}\")\n",
    "    rtol = 1e-2 if is_hip_mi200() else 0.032\n",
    "    if torch.allclose(triton_output, torch_output, atol=0.15, rtol=rtol):\n",
    "        print(\"✅ Triton and Torch match\")\n",
    "    else:\n",
    "        diff = triton_output - torch_output\n",
    "        abs_diff = torch.abs(diff)\n",
    "        max_abs_diff = torch.max(abs_diff)\n",
    "        print(f\"❌ Triton and Torch differ: {max_abs_diff=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8889,
     "status": "ok",
     "timestamp": 1740801796650,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "kj_dGOlazQJY",
    "outputId": "e888057e-fb6d-46ec-e011-d2e94ac0df85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton implementation\n",
      "PyTorch implementation\n",
      "Performance comparison for matrix multiplication (2048x2048 @ 2048x2048):\n",
      "Triton implementation: 0.74 ms\n",
      "PyTorch implementation: 1.01 ms\n",
      "\n",
      "Speedup of Triton vs PyTorch: 1.36x\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Performance Benchmark\n",
    "# IMPORTANT: DO NOT CHANGE THIS CODE.\n",
    "# THIS IS THE EXACT CODE THAT WILL BE USED TO GRADE YOUR IMPLEMENTATION.\n",
    "# ANY CHANGES TO THIS CODE (INCLUDING DIMENSIONS, REPEATS, etc.)\n",
    "# WILL CAUSE YOU TO HAVE DIFFERENT SPEEDUP RESULTS.\n",
    "# -----------------------------------------------------------------------------\n",
    "M = 2048\n",
    "K = 2048\n",
    "N = 2048\n",
    "\n",
    "# KEEP THESE MATRICES IN FP16. FP32 WILL NOT PROVIDE ACCURATE RESULTS\n",
    "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
    "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
    "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "# warmup\n",
    "_ = matmul_add_relu_fp16(A, B, C)\n",
    "_ = reference_matmul_add_relu(A, B, C)\n",
    "\n",
    "REPEATS = 5000\n",
    "\n",
    "# time your implementation\n",
    "print(\"Triton implementation\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "for _ in range(REPEATS):\n",
    "    _ = matmul_add_relu_fp16(A, B, C)\n",
    "torch.cuda.synchronize()\n",
    "triton_time = (time.perf_counter() - start) / REPEATS\n",
    "\n",
    "# time pytorch\n",
    "print(\"PyTorch implementation\")\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "for _ in range(REPEATS):\n",
    "    _ = reference_matmul_add_relu(A, B, C)\n",
    "torch.cuda.synchronize()\n",
    "torch_time = (time.perf_counter() - start) / REPEATS\n",
    "\n",
    "print(f\"Performance comparison for matrix multiplication ({M}x{K} @ {K}x{N}):\")\n",
    "print(f\"Triton implementation: {triton_time*1000:.2f} ms\")\n",
    "print(f\"PyTorch implementation: {torch_time*1000:.2f} ms\")\n",
    "\n",
    "print(f\"\\nSpeedup of Triton vs PyTorch: {torch_time/triton_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 519652,
     "status": "ok",
     "timestamp": 1740803005867,
     "user": {
      "displayName": "Kuber Shahi",
      "userId": "17371841513030871320"
     },
     "user_tz": 480
    },
    "id": "K9Hdpxic0tq6",
    "outputId": "f212d8ea-b1d9-4680-95d2-b7b866d175c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search...\n",
      "PyTorch baseline: 0.95 ms\n",
      "\n",
      "Testing configurations:\n",
      "M=32, N=32, K=16: 4.32 ms (speedup: 0.22x)\n",
      "M=32, N=32, K=32: 4.20 ms (speedup: 0.23x)\n",
      "M=32, N=32, K=64: 3.42 ms (speedup: 0.28x)\n",
      "M=32, N=64, K=16: 2.62 ms (speedup: 0.36x)\n",
      "M=32, N=64, K=32: 2.51 ms (speedup: 0.38x)\n",
      "M=32, N=64, K=64: 2.19 ms (speedup: 0.43x)\n",
      "M=32, N=128, K=16: 1.82 ms (speedup: 0.52x)\n",
      "M=32, N=128, K=32: 1.72 ms (speedup: 0.55x)\n",
      "M=32, N=128, K=64: 1.49 ms (speedup: 0.64x)\n",
      "M=32, N=256, K=16: 1.32 ms (speedup: 0.72x)\n",
      "M=32, N=256, K=32: 1.35 ms (speedup: 0.70x)\n",
      "M=32, N=256, K=64: 1.51 ms (speedup: 0.63x)\n",
      "M=64, N=32, K=16: 3.40 ms (speedup: 0.28x)\n",
      "M=64, N=32, K=32: 3.36 ms (speedup: 0.28x)\n",
      "M=64, N=32, K=64: 2.70 ms (speedup: 0.35x)\n",
      "M=64, N=64, K=16: 2.10 ms (speedup: 0.45x)\n",
      "M=64, N=64, K=32: 1.97 ms (speedup: 0.48x)\n",
      "M=64, N=64, K=64: 1.68 ms (speedup: 0.57x)\n",
      "M=64, N=128, K=16: 1.23 ms (speedup: 0.78x)\n",
      "M=64, N=128, K=32: 1.29 ms (speedup: 0.74x)\n",
      "M=64, N=128, K=64: 1.08 ms (speedup: 0.88x)\n",
      "M=64, N=256, K=16: 0.87 ms (speedup: 1.10x)\n",
      "M=64, N=256, K=32: 0.91 ms (speedup: 1.04x)\n",
      "M=64, N=256, K=64: 1.12 ms (speedup: 0.85x)\n",
      "M=128, N=32, K=16: 2.44 ms (speedup: 0.39x)\n",
      "M=128, N=32, K=32: 2.25 ms (speedup: 0.42x)\n",
      "M=128, N=32, K=64: 1.90 ms (speedup: 0.50x)\n",
      "M=128, N=64, K=16: 1.37 ms (speedup: 0.70x)\n",
      "M=128, N=64, K=32: 1.37 ms (speedup: 0.69x)\n",
      "M=128, N=64, K=64: 1.22 ms (speedup: 0.78x)\n",
      "M=128, N=128, K=16: 0.89 ms (speedup: 1.07x)\n",
      "M=128, N=128, K=32: 0.97 ms (speedup: 0.98x)\n",
      "M=128, N=128, K=64: 0.81 ms (speedup: 1.17x)\n",
      "M=128, N=256, K=16: 0.73 ms (speedup: 1.30x)\n",
      "M=128, N=256, K=32: 0.75 ms (speedup: 1.27x)\n",
      "M=128, N=256, K=64: 0.89 ms (speedup: 1.07x)\n",
      "M=256, N=32, K=16: 1.87 ms (speedup: 0.51x)\n",
      "M=256, N=32, K=32: 1.59 ms (speedup: 0.60x)\n",
      "M=256, N=32, K=64: 1.63 ms (speedup: 0.58x)\n",
      "M=256, N=64, K=16: 1.07 ms (speedup: 0.89x)\n",
      "M=256, N=64, K=32: 1.05 ms (speedup: 0.91x)\n",
      "M=256, N=64, K=64: 1.06 ms (speedup: 0.90x)\n",
      "M=256, N=128, K=16: 0.78 ms (speedup: 1.22x)\n",
      "M=256, N=128, K=32: 0.73 ms (speedup: 1.30x)\n",
      "M=256, N=128, K=64: 0.90 ms (speedup: 1.06x)\n",
      "M=256, N=256, K=16: 1.88 ms (speedup: 0.51x)\n",
      "M=256, N=256, K=32: 4.61 ms (speedup: 0.21x)\n",
      "M=256, N=256, K=64: 20.00 ms (speedup: 0.05x)\n",
      "\n",
      "Best configuration found:\n",
      "BLOCK_M = 256\n",
      "BLOCK_N = 128\n",
      "BLOCK_K = 32\n",
      "Speedup: 1.30x\n"
     ]
    }
   ],
   "source": [
    "# Write your grid search here.\n",
    "\n",
    "# Grid search for optimal block sizes\n",
    "def benchmark_matmul(A, B, C, block_m, block_n, block_k, num_repeats=REPEATS):\n",
    "    global BLOCK_M, BLOCK_N, BLOCK_K\n",
    "    BLOCK_M = block_m\n",
    "    BLOCK_N = block_n\n",
    "    BLOCK_K = block_k\n",
    "\n",
    "    # Warmup\n",
    "    _ = matmul_add_relu_fp16(A, B, C)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # Time Triton implementation\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_repeats):\n",
    "        _ = matmul_add_relu_fp16(A, B, C)\n",
    "    torch.cuda.synchronize()\n",
    "    triton_time = (time.perf_counter() - start) / num_repeats\n",
    "\n",
    "    return triton_time\n",
    "\n",
    "# Generate test matrices\n",
    "M = N = K = 2048\n",
    "A = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
    "B = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
    "C = torch.randn((M, N), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "# Get PyTorch baseline\n",
    "torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "for _ in range(REPEATS):\n",
    "    _ = reference_matmul_add_relu(A, B, C)\n",
    "torch.cuda.synchronize()\n",
    "torch_time = (time.perf_counter() - start) / REPEATS\n",
    "\n",
    "# Define search space (powers of 2)\n",
    "block_m_sizes = [32, 64, 128, 256]\n",
    "block_n_sizes = [32, 64, 128, 256]\n",
    "block_k_sizes = [16, 32, 64]\n",
    "\n",
    "best_speedup = 0\n",
    "best_params = None\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "print(f\"PyTorch baseline: {torch_time*1000:.2f} ms\")\n",
    "print(\"\\nTesting configurations:\")\n",
    "\n",
    "for block_m in block_m_sizes:\n",
    "    for block_n in block_n_sizes:\n",
    "        for block_k in block_k_sizes:\n",
    "            try:\n",
    "                triton_time = benchmark_matmul(A, B, C, block_m, block_n, block_k)\n",
    "                speedup = torch_time / triton_time\n",
    "\n",
    "                print(f\"M={block_m}, N={block_n}, K={block_k}: {triton_time*1000:.2f} ms (speedup: {speedup:.2f}x)\")\n",
    "\n",
    "                if speedup > best_speedup:\n",
    "                    best_speedup = speedup\n",
    "                    best_params = (block_m, block_n, block_k)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with M={block_m}, N={block_n}, K={block_k}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "print(f\"\\nBest configuration found:\")\n",
    "print(f\"BLOCK_M = {best_params[0]}\")\n",
    "print(f\"BLOCK_N = {best_params[1]}\")\n",
    "print(f\"BLOCK_K = {best_params[2]}\")\n",
    "print(f\"Speedup: {best_speedup:.2f}x\")\n",
    "\n",
    "# Set the best parameters\n",
    "BLOCK_M = best_params[0]\n",
    "BLOCK_N = best_params[1]\n",
    "BLOCK_K = best_params[2]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
